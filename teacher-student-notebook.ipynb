{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba682f49b1aaeb86",
   "metadata": {
    "id": "ba682f49b1aaeb86"
   },
   "source": [
    "# Teacher Student Network Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740fcaf00eaf41b",
   "metadata": {
    "id": "e740fcaf00eaf41b"
   },
   "source": [
    "##### Teacher Student Network Research\n",
    "Framework adapted from Official Pytorch Knowledge Distillation Tutorial\n",
    "\n",
    "Author:\n",
    "Asad Amiruddin,\n",
    "Harrison Maximillian Rush,\n",
    "Huy N Ho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e4ff9cde85e16",
   "metadata": {
    "id": "a92e4ff9cde85e16"
   },
   "source": [
    "### Import library, datasets, loaders"
   ]
  },
  {
   "cell_type": "code",
   "id": "e00f96571c1c5a3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e00f96571c1c5a3d",
    "outputId": "f6bffbcb-9210-40ae-97cd-7f9855161b10"
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!git clone https://github.com/pytorch/captum\n",
    "%cd captum\n",
    "!git checkout \"v0.2.0\"\n",
    "!pip3 install -e .\n",
    "import sys\n",
    "sys.path.append('/content/captum')\n",
    "%cd ..\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "from time import time\n",
    "from torchvision import models\n",
    "\n",
    "from captum.attr import Saliency, IntegratedGradients, NoiseTunnel, DeepLift\n",
    "from matplotlib import pyplot as plt\n",
    "from captum.attr import visualization as viz\n",
    "from torchvision import models\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# Loading the CIFAR-10 dataset:\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_dataset_nonorm = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_nonorm)\n",
    "\n",
    "# Dataloaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_loader_nonorm = torch.utils.data.DataLoader(test_dataset_nonorm, batch_size=128, shuffle=False, num_workers=2)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "33af03f44164e9de",
   "metadata": {
    "id": "33af03f44164e9de"
   },
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "148ed29ebe462888",
   "metadata": {
    "id": "148ed29ebe462888"
   },
   "source": [
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    start = time()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "    end = time()\n",
    "    runtime = end - start\n",
    "    print(f\"Training Time: {runtime:.3f}\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    print('Knowledge distillation training')\n",
    "    start = time()\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft target loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "    end = time()\n",
    "    runtime = end - start\n",
    "    print(f\"Training Time: {runtime:.3f}\")\n",
    "\n",
    "def show_saliency_map(model, data_loader,title, img_index=3 ):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    input = images[img_index].unsqueeze(0)\n",
    "    original_image = np.transpose((images[img_index].cpu().detach().numpy() / 2) + 0.5, (1, 2, 0))\n",
    "    input.requires_grad = True\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    saliency = Saliency(model)\n",
    "    gradients = saliency.attribute(input, target=labels[img_index].item())\n",
    "    gradients = np.transpose(gradients.squeeze().cpu().detach().numpy(), (1, 2, 0))\n",
    "    classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    img_label = classes[labels[img_index]]\n",
    "    _ = viz.visualize_image_attr(gradients, original_image, method=\"blended_heat_map\", sign=\"absolute_value\",\n",
    "                              show_colorbar=True, title= title + \" - Overlayed Gradient Magnitudes - \" + img_label )\n",
    "    return None\n",
    "\n",
    "def show_integrated_grad(model, data_loader = test_loader,title = 'Teacher Model', img_index=3 ):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    input = images[img_index].unsqueeze(0)\n",
    "    original_image = np.transpose((images[img_index].cpu().detach().numpy() / 2) + 0.5, (1, 2, 0))\n",
    "    input.requires_grad = True\n",
    "    model.eval()\n",
    "    ig = IntegratedGradients(model)\n",
    "\n",
    "    model.zero_grad()\n",
    "    ig = ig.attribute(input,target=labels[img_index])\n",
    "    ig = np.transpose(ig.squeeze().cpu().detach().numpy(), (1, 2, 0))\n",
    "    classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    img_label = classes[labels[img_index]]\n",
    "    _ = viz.visualize_image_attr(ig, original_image, method=\"blended_heat_map\",sign=\"all\",\n",
    "                              show_colorbar=True, title=\"Overlayed Integrated Gradients - Image #\" + img_label)\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bde8a78adfa4e19",
   "metadata": {
    "id": "bde8a78adfa4e19"
   },
   "source": [
    "### Define deeper neural networks to be used as teachers.\n",
    "Can have multiple teachers for comparison/experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d4aec7840d87",
   "metadata": {
    "id": "fc1d4aec7840d87"
   },
   "source": [
    "### Load resnet50 model with finetuned weight as another teacher"
   ]
  },
  {
   "cell_type": "code",
   "id": "4780ca82b559df31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4780ca82b559df31",
    "outputId": "0bb42181-2931-4346-d3fa-ccb36cff246b"
   },
   "source": [
    "teacher_resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "teacher_resnet50.fc = nn.Linear(teacher_resnet50.fc.in_features, 10)\n",
    "teacher_resnet50 = teacher_resnet50.to(device)\n",
    "teacher_resnet50.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/teacher_resnet50.pth\",map_location=device  ))\n",
    "# teacher_resnet50.load_state_dict(torch.load(\"./trained_model/teacher_resnet50.pth\",map_location=device))\n",
    "test_accuracy_teacher = test(teacher_resnet50, test_loader, device)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bffe046810950e79",
   "metadata": {
    "id": "bffe046810950e79"
   },
   "source": [
    "### Define student network\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cef6b8911ea191b3",
   "metadata": {
    "id": "cef6b8911ea191b3"
   },
   "source": [
    "# Define the student model\n",
    "class studentNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(studentNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Instantiate the model\n",
    "# studentNN = studentNN().to(device)\n",
    "\n",
    "# test_accuracy_student = test(studentNN, test_loader, device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6f9e65a166b4f447",
   "metadata": {
    "id": "6f9e65a166b4f447"
   },
   "source": [
    "### Instantiate the 2 identical student nets"
   ]
  },
  {
   "cell_type": "code",
   "id": "c83345791928e709",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c83345791928e709",
    "outputId": "bec11d47-3f64-471b-ff1f-aaf824b132c4"
   },
   "source": [
    "\n",
    "# Instantiate the lightweight network:\n",
    "# We instantiate one more lightweight network model to compare their performances.\n",
    "# Back propagation is sensitive to weight initialization,\n",
    "# so we need to make sure these two networks have the exact same initialization.\n",
    "torch.manual_seed(42)\n",
    "learning_student = studentNN().to(device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "non_learning_student = studentNN().to(device)\n",
    "\n",
    "# To ensure we have created a copy of the first network, we inspect the norm of its first layer.\n",
    "# If it matches, then we are safe to conclude that the networks are indeed the same.\n",
    "\n",
    "# Print the total number of parameters in each model:\n",
    "total_params_teacher = \"{:,}\".format(sum(p.numel() for p in teacher_resnet50.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_teacher}\")\n",
    "total_params_non_learning_student = \"{:,}\".format(sum(p.numel() for p in non_learning_student.parameters()))\n",
    "print(f\"non_learning_student parameters: {total_params_non_learning_student}\")\n",
    "total_params_learning_student = \"{:,}\".format(sum(p.numel() for p in learning_student.parameters()))\n",
    "print(f\"learning_student parameters: {total_params_learning_student}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fdf4aa2903489743",
   "metadata": {
    "id": "fdf4aa2903489743"
   },
   "source": [
    "### Train the students and compare to the one without teacher"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b7cbc79347a2eed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b7cbc79347a2eed",
    "outputId": "bde89918-47c7-478e-c756-820275c87525"
   },
   "source": [
    "\n",
    "\n",
    "load_non_learning_student_model_from_Gdrive = True   # need to be on Colab\n",
    "load_non_learning_student_model_from_local_hard_drive = False\n",
    "\n",
    "if load_non_learning_student_model_from_Gdrive:\n",
    "    non_learning_student.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/non_learning_student.pth\"  ))\n",
    "elif load_non_learning_student_model_from_local_hard_drive :\n",
    "    non_learning_student.load_state_dict(torch.load(\"./trained_model/non_learning_student.pth\",map_location=device))\n",
    "else:\n",
    "    #train non_learning_student on train dataset\n",
    "    train(non_learning_student, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "\n",
    "\n",
    "load_learning_student_model_from_Gdrive = True   # need to be on Colab\n",
    "load_learning_student_model_from_local_hard_drive = False\n",
    "\n",
    "if load_learning_student_model_from_Gdrive:\n",
    "    learning_student.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/learning_student.pth\"  ))\n",
    "elif load_learning_student_model_from_local_hard_drive :\n",
    "    learning_student.load_state_dict(torch.load(\"./trained_model/learning_student.pth\",map_location=device))\n",
    "else:\n",
    "    # Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "    train_knowledge_distillation(teacher=teacher_resnet50, student=learning_student,\n",
    "                                 train_loader=train_loader, epochs=10, learning_rate=0.001,\n",
    "                                 T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "\n",
    "test_accuracy_learning_student = test(learning_student, test_loader, device)\n",
    "test_accuracy_non_learning = test(non_learning_student, test_loader, device)\n",
    "\n",
    "# Compare the student test accuracy with and without the teacher, after distillation\n",
    "# Result shouldn't be stellar because teacher's prediction can't beat ground truth here\n",
    "print(f\"Teacher accuracy: {test_accuracy_teacher:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_non_learning:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_learning_student:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "20884ad6d26f749",
   "metadata": {
    "id": "20884ad6d26f749"
   },
   "source": [
    "### Save trained models - only run after training on Colab"
   ]
  },
  {
   "cell_type": "code",
   "id": "15b8d4dff728654d",
   "metadata": {
    "id": "15b8d4dff728654d"
   },
   "source": [
    "# IMPORTANT\n",
    "# change the boolean below to True to save the trained model .pth file into your Google Drive\n",
    "# only work if executing on Google Colab\n",
    "save_model_to_Gdrive = False # need to be on Colab\n",
    "save_model_to_local_drive = False\n",
    "if save_model_to_Gdrive:\n",
    "    torch.save(teacher_resnet50.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/teacher_resnet50.pth\")\n",
    "    torch.save(non_learning_student.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/non_learning_student.pth\") # non-learning student\n",
    "    torch.save(learning_student.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/learning_student.pth\") # student after KD\n",
    "elif save_model_to_local_drive:\n",
    "    torch.save(teacher_resnet50.state_dict(), \"./trained_model/teacher_resnet50.pth\")\n",
    "    torch.save(non_learning_student.state_dict(), \"./trained_model/non_learning_student.pth\") # non-learning student\n",
    "    torch.save(learning_student.state_dict(), \"./trained_model/learning_student.pth\") # student after KD\n",
    "\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ff39812e4a4b02b"
   },
   "cell_type": "markdown",
   "source": [
    "### Model interpretation    "
   ],
   "id": "ff39812e4a4b02b"
  },
  {
   "metadata": {
    "id": "b6f0737b5920c1a4"
   },
   "cell_type": "markdown",
   "source": [],
   "id": "b6f0737b5920c1a4"
  },
  {
   "cell_type": "code",
   "id": "829a235c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "829a235c",
    "outputId": "d9afafa3-1e76-45d4-d4e2-8e0740549185"
   },
   "source": [
    "show_saliency_map(model = teacher_resnet50, data_loader = test_loader_nonorm,title = 'Teacher Model', img_index=3)\n",
    "show_saliency_map(model = learning_student, data_loader = test_loader_nonorm,title = 'Student Model', img_index=3)\n",
    "show_integrated_grad(model = teacher_resnet50, data_loader = test_loader_nonorm,title = 'Teacher Model', img_index=3)\n",
    "show_integrated_grad(model = learning_student, data_loader = test_loader_nonorm,title = 'Student Model', img_index=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "070fd7ab",
   "metadata": {
    "id": "070fd7ab"
   },
   "source": [
    "# Resnet and Intermediate Feature Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae347b49",
   "metadata": {
    "id": "ae347b49"
   },
   "source": [
    "Found a pretrained resnet32 on Cifar-10. 93% Accuracy reproduceable\n",
    "https://github.com/chenyaofo/pytorch-cifar-models"
   ]
  },
  {
   "cell_type": "code",
   "id": "0ec37dba",
   "metadata": {
    "id": "0ec37dba"
   },
   "source": [
    "from helper import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3c188c6",
   "metadata": {
    "id": "e3c188c6"
   },
   "source": [
    "We set hooks to provide the activation outputs at the specified layers"
   ]
  },
  {
   "cell_type": "code",
   "id": "199b754b",
   "metadata": {
    "id": "199b754b"
   },
   "source": [
    "resnet32 = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet32\", pretrained=True).to(device)\n",
    "student_inter = studentNN(num_classes=10).to(device)\n",
    "\n",
    "teacher_layer= resnet32.layer2[2]\n",
    "teacher_activations = Hook(teacher_layer)\n",
    "\n",
    "student_layer = student_inter.features[4]\n",
    "student_activations = Hook(student_layer)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60e09a3e",
   "metadata": {
    "id": "60e09a3e",
    "outputId": "d8fb3962-da12-4543-abb1-526f70721a68"
   },
   "source": [
    "test(resnet32, test_loader, device)\n",
    "test(student_inter, test_loader, device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e158b59a",
   "metadata": {
    "id": "e158b59a",
    "outputId": "8cf37a85-eccd-427f-b2dd-12bf68888372"
   },
   "source": [
    "print(student_activations.output.shape)\n",
    "print(teacher_activations.output.shape)\n",
    "print(resnet32.layer2)\n",
    "print(student_inter)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f3f33d1e",
   "metadata": {
    "id": "f3f33d1e"
   },
   "source": [
    "First stage training optimizes up to the hidden layer specified earlier, and uses the loss between teacher and student activations. Afterwards, we continue with the regular training routine"
   ]
  },
  {
   "cell_type": "code",
   "id": "762d9c773c6e8439",
   "metadata": {
    "id": "762d9c773c6e8439",
    "outputId": "1d5dd67a-d280-433d-e5fb-67b07a6d2697"
   },
   "source": [
    "\n",
    "train_first_stage(train_loader, student_inter, resnet32, hint_loss, 10, 0.001)\n",
    "test(student_inter, test_loader, device)\n",
    "train(student_inter, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test(student_inter, test_loader, device)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
