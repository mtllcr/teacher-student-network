{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba682f49b1aaeb86",
   "metadata": {
    "id": "ba682f49b1aaeb86"
   },
   "source": [
    "# Teacher Student - KD using Intermedia Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740fcaf00eaf41b",
   "metadata": {
    "id": "e740fcaf00eaf41b"
   },
   "source": [
    "##### Teacher Student Network Research\n",
    "Framework adapted from Official Pytorch Knowledge Distillation Tutorial\n",
    "\n",
    "Author:\n",
    "Asad Amiruddin,\n",
    "Harrison Maximillian Rush,\n",
    "Huy N Ho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e4ff9cde85e16",
   "metadata": {
    "id": "a92e4ff9cde85e16"
   },
   "source": [
    "### Import library, datasets, loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00f96571c1c5a3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:20:52.292340Z",
     "start_time": "2024-04-29T17:20:48.674091Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e00f96571c1c5a3d",
    "outputId": "b7ca4727-c96f-4dd5-8787-8286096fd2f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# !git clone https://github.com/pytorch/captum\n",
    "# %cd captum\n",
    "# !git checkout \"v0.2.0\"\n",
    "# !pip3 install -e .\n",
    "# import sys\n",
    "# sys.path.append('/content/captum')\n",
    "# %cd ..\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "from time import time\n",
    "from torchvision import models\n",
    "import captum\n",
    "import gc\n",
    "from captum import attr\n",
    "from captum.attr import Saliency, IntegratedGradients, NoiseTunnel, DeepLift, LayerGradCam, LayerAttribution\n",
    "from matplotlib import pyplot as plt\n",
    "from captum.attr import visualization as viz\n",
    "from torchvision import models\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transform_nonorm = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# Loading the CIFAR-10 dataset:\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_dataset_nonorm = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_nonorm)\n",
    "\n",
    "# Dataloaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_loader_nonorm = torch.utils.data.DataLoader(test_dataset_nonorm, batch_size=128, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af03f44164e9de",
   "metadata": {
    "id": "33af03f44164e9de"
   },
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ed29ebe462888",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:20:52.339343Z",
     "start_time": "2024-04-29T17:20:52.292340Z"
    },
    "id": "148ed29ebe462888"
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_improv=0.01):\n",
    "        self.patience = patience\n",
    "        self.min_improv = min_improv\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def check_loss(self, validation_loss):\n",
    "      # if validation loss improve by at least min_improv percentage, then\n",
    "      # set min to current loss and reset the counter\n",
    "        if validation_loss <  (self.min_validation_loss * (1 - self.min_improv)):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "      # else if validation loss exceeds previous loss by the min_improv percentage,\n",
    "      # start counter until hit patience\n",
    "        elif validation_loss > (self.min_validation_loss * (1 + self.min_improv)):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def train(model, train_loader, epochs, learning_rate, device, early_stop = False):\n",
    "    start = time()\n",
    "    early_stopper = EarlyStopper(patience=3, min_improv=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses_epoch = []\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        losses = []\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            running_loss += float(loss.item())\n",
    "        losses_epoch.append(np.mean(losses))\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "        if early_stop and early_stopper.check_loss(running_loss):\n",
    "          break\n",
    "\n",
    "        \n",
    "    end = time()\n",
    "    runtime = end - start\n",
    "    del optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect() \n",
    "    print(f\"Training Time: {runtime:.3f}\")\n",
    "    return losses_epoch\n",
    "\n",
    "def test(model, test_loader, device, to_cpu = False):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    if to_cpu: \n",
    "      model.to('cpu')\n",
    "    inputs, labels = inputs.to('cpu'), labels.to('cpu')\n",
    "    return accuracy\n",
    "\n",
    "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, device, early_stop = False):\n",
    "    print('Knowledge distillation training')\n",
    "    start = time()\n",
    "    early_stopper = EarlyStopper(patience=3, min_improv=0.1)\n",
    "    ce_loss_weight= 1 - soft_target_loss_weight\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "    losses_epoch = []\n",
    "    acc_epoch = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        losses = []\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft target loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        #losses_epoch.append(np.mean(losses))\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "        if early_stop and early_stopper.check_loss(running_loss):\n",
    "          break\n",
    "\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        test_current_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "                outputs = student(inputs)\n",
    "                loss = ce_loss(outputs, labels)\n",
    "                test_current_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "        losses_epoch.append(test_current_loss / len(test_loader))\n",
    "        acc_epoch.append(100 * correct_test / total_test)\n",
    "        print('[%d] test_loss: %.3f, test_accuracy: %.2f %%' %\n",
    "        (epoch + 1, test_current_loss / len(test_loader), 100 * correct_test / total_test))\n",
    "    end = time()\n",
    "    runtime = end - start\n",
    "    print(f\"Training Time: {runtime:.3f}\")\n",
    "    del optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect() \n",
    "    return losses_epoch\n",
    "\n",
    "def show_org_img(model, data_loader,title, img_index=3 ):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    original_image = np.transpose((images[img_index].cpu().detach().numpy() / 2) + 0.5, (1, 2, 0))\n",
    "    classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    img_label = classes[labels[img_index]]\n",
    "    _ = viz.visualize_image_attr(None , original_image, method=\"original_image\",fig_size =(2,2), sign=\"absolute_value\",\n",
    "                              show_colorbar=True, title= title + \" - Overlayed Gradient Magnitudes - \" + img_label )\n",
    "    return None\n",
    "\n",
    "def show_saliency_map(model, data_loader,title, img_index=3 ):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    input = images[img_index].unsqueeze(0)\n",
    "    original_image = np.transpose((images[img_index].cpu().detach().numpy() / 2) + 0.5, (1, 2, 0))\n",
    "    input.requires_grad = True\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    saliency = Saliency(model)\n",
    "    gradients = saliency.attribute(input, target=labels[img_index].item())\n",
    "    gradients = np.transpose(gradients.squeeze().cpu().detach().numpy(), (1, 2, 0))\n",
    "    classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    img_label = classes[labels[img_index]]\n",
    "    _ = viz.visualize_image_attr(gradients, original_image , fig_size =(2,2),\n",
    "                                 method=\"blended_heat_map\", sign=\"absolute_value\",\n",
    "                              show_colorbar=True, title= title + \" - Saliency - \" + img_label )\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def show_integrated_grad(model, data_loader,title , img_index=3 ):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    input = images[img_index].unsqueeze(0)\n",
    "    original_image = np.transpose((images[img_index].cpu().detach().numpy() / 2) + 0.5, (1, 2, 0))\n",
    "    input.requires_grad = True\n",
    "    model.eval()\n",
    "    ig = IntegratedGradients(model)\n",
    "\n",
    "    model.zero_grad()\n",
    "    ig = ig.attribute(input,target=labels[img_index])\n",
    "    ig = np.transpose(ig.squeeze().cpu().detach().numpy(), (1, 2, 0))\n",
    "    classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    img_label = classes[labels[img_index]]\n",
    "    _ = viz.visualize_image_attr(ig, original_image,fig_size =(2,2), method=\"blended_heat_map\", sign=\"all\",\n",
    "                              show_colorbar=True, title= title + \" - Overlayed Integrated Gradients - Image #\" + img_label)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def layer_gradCAM(model, conv_layer, data_loader,title, img_index=3):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    input = images[img_index].unsqueeze(0)\n",
    "    classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    img_label = classes[labels[img_index]]\n",
    "    original_image = np.transpose((images[img_index].cpu().detach().numpy() / 2) + 0.5, (1, 2, 0))\n",
    "    input.requires_grad = True\n",
    "    model.eval()\n",
    "    layer_gradcam = LayerGradCam(model, conv_layer)\n",
    "    attributions_lgc = layer_gradcam.attribute(input, target=img_index.item())\n",
    "    # upsamp_attr_lgc = LayerAttribution.interpolate(attributions_lgc, input.shape[2:])\n",
    "    upsamp_attr_lgc = LayerAttribution.interpolate(attributions_lgc, (224, 224),interpolate_mode= \"nearest\")\n",
    "    upsamp_attr_lgc = upsamp_attr_lgc[0].cpu().permute(1,2,0).detach().numpy()\n",
    "    _ = viz.visualize_image_attr(upsamp_attr_lgc,original_image,fig_size =(2,2),show_colorbar=True, \n",
    "                             sign=\"all\",\n",
    "                             title=  title + \" Layer GradCAM - Image #\" + img_label)\n",
    "    return None\n",
    "\n",
    "def normalize_matrix(matrix):\n",
    "    # Convert the matrix to a numpy array for easier manipulation\n",
    "    matrix = np.array(matrix)\n",
    "\n",
    "    # Calculate the sum of all elements in the matrix\n",
    "    total_sum = np.sum(matrix)\n",
    "\n",
    "    # Normalize the matrix by dividing each element by the total sum\n",
    "    normalized_matrix = matrix / total_sum\n",
    "\n",
    "    return normalized_matrix\n",
    "\n",
    "\n",
    "def weighted_average_distance(matrix):\n",
    "    # Normalize the matrix\n",
    "    normalized_matrix = normalize_matrix(matrix)\n",
    "\n",
    "    # Calculate the weighted center\n",
    "    total_weight = np.sum(normalized_matrix)\n",
    "    weighted_center_x = np.sum(normalized_matrix * np.arange(normalized_matrix.shape[0])[:, np.newaxis]) / total_weight\n",
    "    weighted_center_y = np.sum(normalized_matrix * np.arange(normalized_matrix.shape[1])[np.newaxis, :]) / total_weight\n",
    "\n",
    "    # Calculate the weighted distances\n",
    "    weighted_distances = np.zeros_like(normalized_matrix, dtype=float)\n",
    "    for i in range(normalized_matrix.shape[0]):\n",
    "        for j in range(normalized_matrix.shape[1]):\n",
    "            weighted_distances[i, j] = normalized_matrix[i, j] * np.sqrt(\n",
    "                (i - weighted_center_x) ** 2 + (j - weighted_center_y) ** 2)\n",
    "\n",
    "    # Calculate the weighted average distance\n",
    "    weighted_average_distance = np.sum(weighted_distances) / total_weight\n",
    "\n",
    "    return weighted_average_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8a78adfa4e19",
   "metadata": {
    "id": "bde8a78adfa4e19"
   },
   "source": [
    "### Define deeper neural networks to be used as teachers.\n",
    "Can have multiple teachers for comparison/experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d4aec7840d87",
   "metadata": {
    "id": "fc1d4aec7840d87"
   },
   "source": [
    "### Load resnet50 model with finetuned weight as another teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4780ca82b559df31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:21:02.636394Z",
     "start_time": "2024-04-29T17:20:52.340765Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4780ca82b559df31",
    "outputId": "c165a9ac-0d04-423d-e239-249573316724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.67%\n"
     ]
    }
   ],
   "source": [
    "teacher_resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "teacher_resnet50.fc = nn.Linear(teacher_resnet50.fc.in_features, 10)\n",
    "student_kd_inter_combinedteacher_resnet50 = teacher_resnet50.to(device)\n",
    "#teacher_resnet50.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/teacher_resnet50.pth\",map_location=device  ))\n",
    "teacher_resnet50.load_state_dict(torch.load(\"./trained_model/teacher_resnet50.pth\",map_location=device))\n",
    "test_accuracy_teacher = test(teacher_resnet50, test_loader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe046810950e79",
   "metadata": {
    "id": "bffe046810950e79"
   },
   "source": [
    "### Define student network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef6b8911ea191b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:21:02.673536Z",
     "start_time": "2024-04-29T17:21:02.636394Z"
    },
    "id": "cef6b8911ea191b3"
   },
   "outputs": [],
   "source": [
    "# Define the student model\n",
    "# class studentNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(studentNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "#         self.fc2 = nn.Linear(512, 10)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(self.relu(self.conv1(x)))\n",
    "#         x = self.pool(self.relu(self.conv2(x)))\n",
    "#         x = self.pool(self.relu(self.conv3(x)))\n",
    "#         x = x.view(-1, 128 * 28 * 28)\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# Define another student - this is convNet_Tiny\n",
    "class studentNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(studentNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.conv3(x)  # Direct convolution\n",
    "        x = self.relu(x)   # Activation\n",
    "        x = self.pool2(x)   # Pooling\n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Instantiate the model\n",
    "# studentNN = studentNN().to(device)\n",
    "# test_accuracy_student = test(studentNN, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e65a166b4f447",
   "metadata": {
    "id": "6f9e65a166b4f447"
   },
   "source": [
    "### Instantiate the 2 identical student nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83345791928e709",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:21:02.847809Z",
     "start_time": "2024-04-29T17:21:02.674087Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c83345791928e709",
    "outputId": "1327bdb8-ecbc-460a-b03f-dcbddc8978c8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the lightweight network:\n",
    "# We instantiate one more lightweight network model to compare their performances.\n",
    "# Back propagation is sensitive to weight initialization,\n",
    "# so we need to make sure these two networks have the exact same initialization.\n",
    "torch.manual_seed(42)\n",
    "learning_student = studentNN().to(device)\n",
    "\n",
    "\n",
    "non_learning_student = studentNN().to(device)\n",
    "\n",
    "# To ensure we have created a copy of the first network, we inspect the norm of its first layer.\n",
    "# If it matches, then we are safe to conclude that the networks are indeed the same.\n",
    "\n",
    "# Print the total number of parameters in each model:\n",
    "total_params_teacher = \"{:,}\".format(sum(p.numel() for p in teacher_resnet50.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_teacher}\")\n",
    "total_params_non_learning_student = \"{:,}\".format(sum(p.numel() for p in non_learning_student.parameters()))\n",
    "print(f\"non_learning_student parameters: {total_params_non_learning_student}\")\n",
    "total_params_learning_student = \"{:,}\".format(sum(p.numel() for p in learning_student.parameters()))\n",
    "print(f\"learning_student parameters: {total_params_learning_student}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4aa2903489743",
   "metadata": {
    "id": "fdf4aa2903489743"
   },
   "source": [
    "### Train the students and compare to the one without teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7cbc79347a2eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:21:16.850109Z",
     "start_time": "2024-04-29T17:21:02.847809Z"
    },
    "id": "5b7cbc79347a2eed"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_non_learning_student_model_from_Gdrive = False   # need to be on Colab\n",
    "load_non_learning_student_model_from_local_hard_drive = True\n",
    "\n",
    "if load_non_learning_student_model_from_Gdrive:\n",
    "    non_learning_student.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/non_learning_student.pth\"  ))\n",
    "elif load_non_learning_student_model_from_local_hard_drive :\n",
    "    non_learning_student.load_state_dict(torch.load(\"./trained_model/non_learning_student.pth\",map_location=device))\n",
    "else:\n",
    "    #train non_learning_student on train dataset\n",
    "    train(non_learning_student, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "\n",
    "\n",
    "load_learning_student_model_from_Gdrive = False   # need to be on Colab\n",
    "load_learning_student_model_from_local_hard_drive = True\n",
    "\n",
    "if load_learning_student_model_from_Gdrive:\n",
    "    learning_student.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/learning_student.pth\"  ))\n",
    "elif load_learning_student_model_from_local_hard_drive :\n",
    "    learning_student.load_state_dict(torch.load(\"./trained_model/learning_student.pth\",map_location=device))\n",
    "else:\n",
    "    # Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "    train_knowledge_distillation(teacher=teacher_resnet50, student=learning_student,\n",
    "                                 train_loader=train_loader, epochs=10, learning_rate=0.0005,\n",
    "                                 T=3, soft_target_loss_weight=0.3, device=device)\n",
    "\n",
    "test_accuracy_non_learning = test(non_learning_student, test_loader, device)\n",
    "test_accuracy_learning_student = test(learning_student, test_loader, device)\n",
    "\n",
    "# Compare the student test accuracy with and without the teacher, after distillation\n",
    "# Result shouldn't be stellar because teacher's prediction can't beat ground truth here\n",
    "print(f\"Teacher accuracy: {test_accuracy_teacher:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_non_learning:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_learning_student:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ajbFFvae2x9P",
   "metadata": {
    "id": "ajbFFvae2x9P"
   },
   "source": [
    "###Hyper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H9_IF4T30UDa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:21:16.884572Z",
     "start_time": "2024-04-29T17:21:16.850109Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9_IF4T30UDa",
    "outputId": "04e8356a-af69-4785-f1ce-d988cb1c8522"
   },
   "outputs": [],
   "source": [
    "# hyper parameters tuning\n",
    "# lr_list = [0.002, 0.001, 0.0005]\n",
    "# T_list = [1, 2, 3]\n",
    "# soft_target_loss_weight_list = [0.1,  0.25, 0.3]\n",
    "# \n",
    "# for lr in lr_list:\n",
    "#   for T in T_list:\n",
    "#     for stl in soft_target_loss_weight_list:\n",
    "#       print('Learning Rate - T - Soft Target Loss Weight', lr, T, stl)\n",
    "#       train_knowledge_distillation(teacher=teacher_resnet50, student=learning_student,\n",
    "#                                  train_loader=train_loader, epochs=10, learning_rate=lr,\n",
    "#                                  T=T, soft_target_loss_weight=stl, device=device, early_stop = True)\n",
    "#       test_accuracy_learning_student = test(learning_student, test_loader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20884ad6d26f749",
   "metadata": {
    "id": "20884ad6d26f749"
   },
   "source": [
    "### Save trained models - only run after training on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8d4dff728654d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:21:16.921083Z",
     "start_time": "2024-04-29T17:21:16.884572Z"
    },
    "id": "15b8d4dff728654d"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT\n",
    "# change the boolean below to True to save the trained model .pth file into your Google Drive\n",
    "# only work if executing on Google Colab\n",
    "save_model_to_Gdrive = False # need to be on Colab\n",
    "save_model_to_local_drive = False\n",
    "if save_model_to_Gdrive:\n",
    "    torch.save(teacher_resnet50.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/teacher_resnet50.pth\")\n",
    "    torch.save(non_learning_student.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/non_learning_student.pth\") # non-learning student\n",
    "    torch.save(learning_student.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/learning_student.pth\") # student after KD\n",
    "elif save_model_to_local_drive:\n",
    "    torch.save(teacher_resnet50.state_dict(), \"./trained_model/teacher_resnet50.pth\")\n",
    "    torch.save(non_learning_student.state_dict(), \"./trained_model/non_learning_student.pth\") # non-learning student\n",
    "    torch.save(learning_student.state_dict(), \"./trained_model/learning_student.pth\") # student after KD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c003e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:21:17.154553Z",
     "start_time": "2024-04-29T17:21:16.921083Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(non_learning_student.state_dict(), \"./trained_model/non_learning_studen_test.pth\") # non-learning student\n",
    "torch.save(learning_student.state_dict(), \"./trained_model/learning_student_test.pth\") # student after KD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0737b5920c1a4",
   "metadata": {
    "id": "b6f0737b5920c1a4"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "070fd7ab",
   "metadata": {
    "id": "070fd7ab"
   },
   "source": [
    "# Resnet and Intermediate Feature Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae347b49",
   "metadata": {
    "id": "ae347b49"
   },
   "source": [ 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec37dba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:21:17.189860Z",
     "start_time": "2024-04-29T17:21:17.154553Z"
    },
    "id": "0ec37dba"
   },
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c188c6",
   "metadata": {
    "id": "e3c188c6"
   },
   "source": [
    "We set hooks to provide the activation outputs at the specified layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199b754b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:21:17.429916Z",
     "start_time": "2024-04-29T17:21:17.190734Z"
    },
    "id": "199b754b"
   },
   "outputs": [],
   "source": [
    "student_learner = studentNN().to(device)\n",
    "student_non_learner = studentNN().to(device)\n",
    "#student_first_stage = studentNN().to(device)\n",
    "#student_first_stage.load_state_dict(torch.load(\"./trained_model/first_stage_student.pth\"))\n",
    "student_kd_inter_combined = studentNN().to(device)\n",
    "\n",
    "teacher_layer= teacher_resnet50.layer3[0]\n",
    "#teacher_activations = Hook(teacher_layer)\n",
    "\n",
    "student_layer_kd_combine = student_kd_inter_combined.pool2\n",
    "\n",
    "#student_activations = Hook(student_layer_kd_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e158b59a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:21:17.465506Z",
     "start_time": "2024-04-29T17:21:17.429916Z"
    },
    "id": "e158b59a"
   },
   "outputs": [],
   "source": [
    "#print(student_activations.output.shape)\n",
    "#print(teacher_activations.output.shape)\n",
    "#print(teacher_resnet50.layer3[0])\n",
    "#print(student_learner.conv3)\n",
    "print(teacher_layer)\n",
    "print(student_layer_kd_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c1f8531dd9e376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T17:43:35.034924Z",
     "start_time": "2024-04-29T17:21:17.465506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge distillation training\n",
      "Hook Set: Bottleneck(\n",
      "  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (downsample): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Hook Set: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Epoch 1/50, Loss: 1.567228111464654\n",
      "[1] test_loss: 0.705, test_accuracy: 78.62 %\n",
      "Epoch 2/50, Loss: 1.4858640395771816\n",
      "[2] test_loss: 0.711, test_accuracy: 78.90 %\n",
      "Epoch 3/50, Loss: 1.4654478206658912\n",
      "[3] test_loss: 0.703, test_accuracy: 78.93 %\n",
      "Epoch 4/50, Loss: 1.4534709109064867\n",
      "[4] test_loss: 0.676, test_accuracy: 78.99 %\n",
      "Epoch 5/50, Loss: 1.4447219707167056\n",
      "[5] test_loss: 0.688, test_accuracy: 78.92 %\n",
      "Epoch 6/50, Loss: 1.4331632849505491\n",
      "[6] test_loss: 0.677, test_accuracy: 78.99 %\n",
      "Epoch 7/50, Loss: 1.4268949717816795\n",
      "[7] test_loss: 0.689, test_accuracy: 78.95 %\n",
      "Epoch 8/50, Loss: 1.4157816933853853\n",
      "[8] test_loss: 0.671, test_accuracy: 79.14 %\n",
      "Epoch 9/50, Loss: 1.411064227218823\n",
      "[9] test_loss: 0.672, test_accuracy: 79.30 %\n",
      "Epoch 10/50, Loss: 1.4095167071008317\n",
      "[10] test_loss: 0.701, test_accuracy: 78.72 %\n",
      "Epoch 11/50, Loss: 1.3944736699314069\n",
      "[11] test_loss: 0.660, test_accuracy: 79.93 %\n",
      "Epoch 12/50, Loss: 1.3933983763770375\n",
      "[12] test_loss: 0.654, test_accuracy: 79.64 %\n",
      "Epoch 13/50, Loss: 1.3796743172818742\n",
      "[13] test_loss: 0.671, test_accuracy: 79.27 %\n",
      "Epoch 14/50, Loss: 1.3779699256657945\n",
      "[14] test_loss: 0.667, test_accuracy: 79.59 %\n",
      "Epoch 15/50, Loss: 1.3638137114017517\n",
      "[15] test_loss: 0.666, test_accuracy: 79.58 %\n"
     ]
    }
   ],
   "source": [
    "student_layer_kd_combine = student_kd_inter_combined.pool2\n",
    "\n",
    "loss_comb = train_kd_intermediate_combined(teacher=teacher_resnet50, \n",
    "                               student=student_kd_inter_combined,\n",
    "                                train_loader=train_loader, \n",
    "                                epochs=50, \n",
    "                                learning_rate=0.0005,\n",
    "                                T=3, \n",
    "                                soft_target_loss_weight=0.3, \n",
    "                                device=device, \n",
    "                                criterion_hint=hint_loss, \n",
    "                                student_layer=student_layer_kd_combine, \n",
    "                                teacher_layer=teacher_layer,\n",
    "                                test_loader=test_loader,\n",
    "                                early_stop=True)\n",
    "\n",
    "kd_combine_results = test(student_kd_inter_combined, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc8ba4cede928a",
   "metadata": {},
   "source": [
    "###hyper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15225d910bb448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T01:12:08.428615Z",
     "start_time": "2024-04-29T17:54:53.403508Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyper parameters tuning\n",
    "lr_list = [0.002, 0.001, 0.0008, 0.0005]\n",
    "T_list = [1, 2, 3]\n",
    "soft_target_loss_weight_list = [0.1,  0.25, 0.5, 0.7]\n",
    "\n",
    "\n",
    "for lr in lr_list:\n",
    "  for T in T_list:\n",
    "    for stl in soft_target_loss_weight_list:\n",
    "      print('Learning Rate - T - Soft Target Loss Weight', lr, T, stl)\n",
    "      loss_comb = train_kd_intermediate_combined(teacher=teacher_resnet50, \n",
    "                               student=student_kd_inter_combined,\n",
    "                                train_loader=train_loader, \n",
    "                                epochs=10, \n",
    "                                learning_rate=lr,\n",
    "                                T=T, \n",
    "                                soft_target_loss_weight=stl, \n",
    "                                device=device, \n",
    "                                criterion_hint=hint_loss, \n",
    "                                student_layer=student_layer_kd_combine, \n",
    "                                teacher_layer=teacher_layer,\n",
    "                                early_stop=True)\n",
    "      kd_combine_results = test(student_kd_inter_combined, test_loader, device)\n",
    "\n",
    "torch.save(student_kd_inter_combined.state_dict(), \"./trained_model/student_kd_inter_combined.pth\") # student after KD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470951ed0664788",
   "metadata": {},
   "source": [
    "###Visualize the student model intermediate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3dd41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T05:03:46.018328Z",
     "start_time": "2024-04-28T05:01:32.300701Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "student_kd_inter_combined.load_state_dict(torch.load(\"./feature_transfer_models/combined_kd_inter.pth\"))\n",
    "img_index_list = np.arange(0,9) \n",
    "avg_distance_learning_student = np.empty((0,)) \n",
    "for img_index in img_index_list:  \n",
    "    show_org_img(model = student_kd_inter_combined, data_loader = test_loader_nonorm,title = 'Original Image', img_index=img_index) \n",
    "    \n",
    "    saliency_learning_student = show_saliency_map(model = student_kd_inter_combined, data_loader = test_loader_nonorm,title = 'Student Model', img_index=img_index) \n",
    "    \n",
    "    show_integrated_grad(model = student_kd_inter_combined, data_loader = test_loader_nonorm,title = 'Student Model', img_index=img_index) \n",
    "     \n",
    "    model = student_kd_inter_combined\n",
    "    conv_layer = model.conv3\n",
    "    layer_gradCAM(model = model, conv_layer = conv_layer, data_loader = test_loader_nonorm, title = 'Student Model -  Conv 3', img_index=img_index)\n",
    "      \n",
    "    saliency_learning_student = saliency_learning_student.mean(axis = (2))\n",
    "    saliency_learning_student_mean = weighted_average_distance(saliency_learning_student)\n",
    "    avg_distance_learning_student = np.append(avg_distance_learning_student,saliency_learning_student_mean) \n",
    "      \n",
    "print(\"Weighted Average Distance Learning:\", avg_distance_learning_student) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385a3c480190b8f",
   "metadata": {},
   "source": [
    "###Model resilience to disruption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6b1d52782d881",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aeed82d378249a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T15:26:37.161217Z",
     "start_time": "2024-04-28T15:25:04.639573Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "noise_type = (transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "              transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "              transforms.AugMix(severity= 2,mixture_width=2, chain_depth  = - 1,\n",
    "                                alpha  = 1.0, all_ops  = True,\n",
    "                                interpolation = InterpolationMode.BILINEAR,\n",
    "                                fill = None))\n",
    "\n",
    "for noise in noise_type:\n",
    "  print(noise)\n",
    "\n",
    "\n",
    "  transform_noisy = transforms.Compose([\n",
    "          transforms.Resize(256),\n",
    "          transforms.CenterCrop(224),\n",
    "          noise,\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "      ])\n",
    "  test_dataset_noisy = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_noisy)\n",
    "  test_loader_noisy = torch.utils.data.DataLoader(test_dataset_noisy, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    " \n",
    "  test_accuracy_learning_student = test(student_kd_inter_combined, test_loader_noisy, device)\n",
    " \n",
    " \n",
    "  print(f\"Student accuracy : {test_accuracy_learning_student:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282e49f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T15:31:57.973649Z",
     "start_time": "2024-04-27T15:31:57.733691Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(student_kd_inter_combined.state_dict(), \"./feature_transfer_models/combined_kd_inter.pth\")\n",
    "student_kd_inter_combined.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac87ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T15:45:16.919694Z",
     "start_time": "2024-04-27T15:31:57.974651Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_reg = train(student_non_learner, train_loader, epochs=50, learning_rate=0.001, device=device)\n",
    "non_learner_results = test(student_non_learner, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a9679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T15:45:17.071391Z",
     "start_time": "2024-04-27T15:45:16.919694Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(student_non_learner.state_dict(), \"./feature_transfer_models/student_non_learner.pth\")\n",
    "student_non_learner.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4756f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:06:38.596508Z",
     "start_time": "2024-04-27T15:45:17.071391Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_kd = train_knowledge_distillation(teacher=teacher_resnet50, student=student_learner,\n",
    "                                 train_loader=train_loader, epochs=50, learning_rate=0.0005,\n",
    "                                 T=3, soft_target_loss_weight=0.3, device=device)\n",
    "learner_results__ = test(student_learner, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b865d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:06:38.751873Z",
     "start_time": "2024-04-27T16:06:38.597334Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(student_learner.state_dict(), \"./feature_transfer_models/student_learner.pth\")\n",
    "student_learner.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36416cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:06:38.786990Z",
     "start_time": "2024-04-27T16:06:38.751873Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f8acb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:06:38.872119Z",
     "start_time": "2024-04-27T16:06:38.787323Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "del student_learner\n",
    "del student_non_learner\n",
    "del student_kd_inter_combined\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear memory cache\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4987ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:06:38.906278Z",
     "start_time": "2024-04-27T16:06:38.872119Z"
    }
   },
   "outputs": [],
   "source": [
    "total_losses = []\n",
    "total_losses.append(loss_comb)\n",
    "total_losses.append(loss_reg)\n",
    "total_losses.append(loss_kd)\n",
    "acc = []\n",
    "acc.append(kd_combine_results)\n",
    "acc.append(learner_results__)\n",
    "acc.append(non_learner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d22b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:06:38.940475Z",
     "start_time": "2024-04-27T16:06:38.906278Z"
    }
   },
   "outputs": [],
   "source": [
    "print(total_losses)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed2216e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:06:39.054022Z",
     "start_time": "2024-04-27T16:06:38.940551Z"
    }
   },
   "outputs": [],
   "source": [
    "student_first_stage = studentNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bcfebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:29:15.807182Z",
     "start_time": "2024-04-27T16:06:39.054333Z"
    }
   },
   "outputs": [],
   "source": [
    "student_first_stage = studentNN().to(device)\n",
    "student_layer_first_stage = student_first_stage.pool2\n",
    "loss_first_stage = train_first_stage(train_loader, \n",
    "                                    student_first_stage, \n",
    "                                    teacher_resnet50, \n",
    "                                    hint_loss,\n",
    "                                    epochs=50, \n",
    "                                    learning_rate=0.001, \n",
    "                                     student_layer=student_layer_first_stage, \n",
    "                                      teacher_layer=teacher_layer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d314d3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-27T16:29:15.808184Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "loss_split_kd = train_knowledge_distillation(teacher=teacher_resnet50, student=student_first_stage,\n",
    "                                 train_loader=train_loader, epochs=50, learning_rate=0.0005,\n",
    "                                 T=3, soft_target_loss_weight=0.3, device=device)\n",
    "first_stage_results = test(student_first_stage, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066add28",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "torch.save(student_first_stage.state_dict(), \"./feature_transfer_models/student_2_stage.pth\")\n",
    "student_first_stage.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce30d6",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Student accuracy without teacher: {non_learner_results:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {learner_results__:.2f}%\")\n",
    "print(f\"Student accuracy with First Stage: {first_stage_results:.2f}%\")\n",
    "print(f\"Student accuracy with Combined KD: {kd_combine_results:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29551f4f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "total_losses = []\n",
    "total_losses.append(loss_comb)\n",
    "total_losses.append(loss_reg)\n",
    "total_losses.append(loss_kd)\n",
    "total_losses.append(loss_split_kd)\n",
    "acc = []\n",
    "acc.append(kd_combine_results)\n",
    "acc.append(learner_results__)\n",
    "acc.append(non_learner_results)\n",
    "acc.append(first_stage_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63036e23",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(total_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed5281",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_names = ['Combined Training', 'Regular Training', 'CE Loss KD', 'Split Training']\n",
    "\n",
    "total_losses_norm = normalize_losses(total_losses)\n",
    "plot_normalized_losses(total_losses_norm, model_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac57f5",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298a73f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(acc)\n",
    "plot_model_accuracies(model_names, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74fb7fa",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "student_test_first_stage = studentNN().to(device)\n",
    "student_layer = student_test_first_stage.pool2\n",
    "test_hook_s = Hook(student_layer)\n",
    "test_hook_t = Hook(teacher_layer)\n",
    "acc = test(student_test_first_stage, test_loader, device)\n",
    "print(acc)\n",
    "acc = test(teacher_resnet50, test_loader, device)\n",
    "print(acc)\n",
    "print(test_hook_t.output.shape)\n",
    "regressor_test = ConvolutionalRegressor2().to(device)\n",
    "x = regressor_test(test_hook_t.output)\n",
    "print(x.shape)\n",
    "print(test_hook_s.output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74b419",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "student_test_first_stage = studentNN().to(device)\n",
    "train_first_stage(train_loader, \n",
    "                  student_test_first_stage, \n",
    "                  teacher_resnet50, \n",
    "                  hint_loss, \n",
    "                  10, \n",
    "                  learning_rate=0.001, \n",
    "                  student_layer=student_layer_kd_combine, \n",
    "                  teacher_layer=teacher_layer)\n",
    "test(student_test_first_stage, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639385e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c41a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_losses = [0.1, 0.3, 0.5, 0.7]\n",
    "inter_losses = [0.1, 0.3, 0.5, 0.7]\n",
    "\n",
    "results_list = []\n",
    "params = []\n",
    "\n",
    "for soft in soft_losses:\n",
    "    for inter in inter_losses: \n",
    "        student_kd_inter_combined_tuning = studentNN().to(device)\n",
    "        loss_comb = train_kd_intermediate_combined(teacher=teacher_resnet50, \n",
    "                                    student=student_kd_inter_combined,\n",
    "                                        train_loader=train_loader, \n",
    "                                        epochs=50, \n",
    "                                        learning_rate=0.0005,\n",
    "                                        T=3, \n",
    "                                        soft_target_loss_weight=0.3, \n",
    "                                        device=device, \n",
    "                                        criterion_hint=hint_loss, \n",
    "                                        student_layer=student_layer_kd_combine, \n",
    "                                        teacher_layer=teacher_layer,\n",
    "                                        test_loader=test_loader,\n",
    "                                        early_stop=True)\n",
    "\n",
    "        results = test(student_kd_inter_combined_tuning, test_loader, device)\n",
    "        results_list.append(results)\n",
    "        params.append((soft, inter))\n",
    "        student_kd_inter_combined_tuning.to('cpu')\n",
    "        del student_kd_inter_combined_tuning\n",
    "        print(f'soft loss {soft}, inter_loss {inter} acc {results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f33d1e",
   "metadata": {
    "id": "f3f33d1e"
   },
   "source": [
    "First stage training optimizes up to the hidden layer specified earlier, and uses the loss between teacher and student activations. Afterwards, we continue with the regular training routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7df784",
   "metadata": {},
   "source": [
    "## RESNET18 Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97394e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390fa3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "T = 3\n",
    "soft_target_loss_weight = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75cd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T17:01:13.960020Z",
     "start_time": "2024-04-26T17:01:13.819568Z"
    }
   },
   "outputs": [],
   "source": [
    "student = models.resnet18()\n",
    "student.fc = nn.Linear(student.fc.in_features, 10)\n",
    "student = student.to(device)\n",
    "for param in student.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec150573",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = models.resnet50()\n",
    "teacher.fc = nn.Linear(teacher.fc.in_features, 10)\n",
    "teacher.load_state_dict(torch.load('trained_model/teacher_finetuned_cifar10_v2.pth'))\n",
    "teacher = teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea192e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_kd = nn.KLDivLoss(reduction='batchmean')\n",
    "optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
